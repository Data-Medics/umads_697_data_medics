{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = ['vectorizer_countVec']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters\n",
    "random_seed = 42\n",
    "upstream = {\n",
    "    \"vectorizer_countVec\": {\n",
    "        \"nb\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\vectorizer_countVec.ipynb\",\n",
    "        \"vectorizer\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\vectorizer_countVec.pkl\",\n",
    "    }\n",
    "}\n",
    "product = {\n",
    "    \"nb\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\topic_modeling_disaster_type_final.ipynb\",\n",
    "    \"lda_model_earthquake\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\lda_model_earthquake.pkl\",\n",
    "    \"lda_model_fire\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\lda_model_fire.pkl\",\n",
    "    \"lda_model_flood\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\lda_model_flood.pkl\",\n",
    "    \"lda_model_hurricane\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\lda_model_hurricane.pkl\",\n",
    "    \"lda_topics_disaster_type\": \"C:\\\\Users\\\\gillrobe\\\\DataScience\\\\umads_697_data_medics\\\\pipeline\\\\output\\\\lda_topics_disaster_type.csv\",\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "importing all the functions defined in tools_rjg.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tools_rjg import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "nltk.download('punkt')\n",
    "from gensim.corpora import Dictionary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import (precision_recall_curve,PrecisionRecallDisplay)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# + tags=[\"parameters\"]\n",
    "upstream = []\n",
    "random_seed = 42"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all = generate_disaster_type_dataframe(disaster_types = ('earthquake', 'fire', 'flood', 'hurricane')\n",
    "                                          , dev_train_test= ('dev', 'train', 'test'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all['tweet_text_cleaned'] = df_all['tweet_text'].apply(lambda x: tweet_preprocessing(x))\n",
    "df_all['lemmatized'] = df_all['tweet_text_cleaned'].apply(lambda x: lemmatize_tweet_text(x, allowed_postags=('NOUN', 'ADJ', 'VERB', 'ADV')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all.sample(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_earthquake = {'n_components' : 3,\n",
    "          'max_iter' : 10,\n",
    "          'learning_method' : 'online',\n",
    "          'random_state' : random_seed,\n",
    "          'batch_size' : 128,\n",
    "          'evaluate_every' : -1,\n",
    "          'n_jobs' : 1,\n",
    "          'learning_decay' : .5}\n",
    "\n",
    "params_fire = {'n_components' : 3,\n",
    "          'max_iter' : 10,\n",
    "          'learning_method' : 'online',\n",
    "          'random_state' : random_seed,\n",
    "          'batch_size' : 128,\n",
    "          'evaluate_every' : -1,\n",
    "          'n_jobs' : 1,\n",
    "          'learning_decay' : .5}\n",
    "\n",
    "params_flood = {'n_components' : 3,\n",
    "          'max_iter' : 10,\n",
    "          'learning_method' : 'online',\n",
    "          'random_state' : random_seed,\n",
    "          'batch_size' : 128,\n",
    "          'evaluate_every' : -1,\n",
    "          'n_jobs' : 1,\n",
    "          'learning_decay' : .5}\n",
    "\n",
    "params_hurricane = {'n_components' : 3,\n",
    "          'max_iter' : 10,\n",
    "          'learning_method' : 'online',\n",
    "          'random_state' : random_seed,\n",
    "          'batch_size' : 128,\n",
    "          'evaluate_every' : -1,\n",
    "          'n_jobs' : 1,\n",
    "          'learning_decay' : .5}\n",
    "\n",
    "lda_model_earthquake = LatentDirichletAllocation(**params_earthquake)\n",
    "lda_model_fire = LatentDirichletAllocation(**params_fire)\n",
    "lda_model_flood = LatentDirichletAllocation(**params_flood)\n",
    "lda_model_hurricane = LatentDirichletAllocation(**params_hurricane)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"output\\\\vectorizer_countVec.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "earthquake_text_vectorized = vectorizer.transform(list(df_all[df_all['disaster_type']=='earthquake']['lemmatized']))\n",
    "fire_text_vectorized = vectorizer.transform(list(df_all[df_all['disaster_type']=='fire']['lemmatized']))\n",
    "flood_text_vectorized = vectorizer.transform(list(df_all[df_all['disaster_type']=='flood']['lemmatized']))\n",
    "hurricane_text_vectorized = vectorizer.transform(list(df_all[df_all['disaster_type']=='hurricane']['lemmatized']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model_earthquake.fit_transform(earthquake_text_vectorized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model_fire.fit_transform(fire_text_vectorized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model_flood.fit_transform(flood_text_vectorized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model_hurricane.fit_transform(hurricane_text_vectorized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(product['lda_model_earthquake'], \"wb\") as f:\n",
    "    pickle.dump(lda_model_earthquake, f)\n",
    "with open(product['lda_model_fire'], \"wb\") as f:\n",
    "    pickle.dump(lda_model_fire, f)\n",
    "with open(product['lda_model_flood'], \"wb\") as f:\n",
    "    pickle.dump(lda_model_flood, f)\n",
    "with open(product['lda_model_hurricane'], \"wb\") as f:\n",
    "    pickle.dump(lda_model_hurricane, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood Earthquake: \", lda_model_earthquake.score(earthquake_text_vectorized))\n",
    "print(\"Log Likelihood Fire: \", lda_model_fire.score(fire_text_vectorized))\n",
    "print(\"Log Likelihood Flood: \", lda_model_flood.score(flood_text_vectorized))\n",
    "print(\"Log Likelihood Hurricane: \", lda_model_hurricane.score(hurricane_text_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity Earthquake: \", lda_model_earthquake.perplexity(earthquake_text_vectorized))\n",
    "print(\"Perplexity Fire: \", lda_model_fire.perplexity(fire_text_vectorized))\n",
    "print(\"Perplexity Flood: \", lda_model_flood.perplexity(flood_text_vectorized))\n",
    "print(\"Perplexity Hurricane: \", lda_model_hurricane.perplexity(hurricane_text_vectorized))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Topic Keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics_earthquake = pd.DataFrame(show_topics(fitted_vectorizer=vectorizer, fitted_lda_model=lda_model_earthquake, n_words=100, dname='earthquake'))\n",
    "topics_fire = pd.DataFrame(show_topics(fitted_vectorizer=vectorizer, fitted_lda_model=lda_model_fire, n_words=100, dname='fire'))\n",
    "topics_flood = pd.DataFrame(show_topics(fitted_vectorizer=vectorizer, fitted_lda_model=lda_model_flood, n_words=100, dname='flood'))\n",
    "topics_hurricane = pd.DataFrame(show_topics(fitted_vectorizer=vectorizer, fitted_lda_model=lda_model_hurricane, n_words=100, dname='hurricane'))\n",
    "\n",
    "df_topic_keywords = pd.concat([topics_earthquake,topics_fire,topics_flood,topics_hurricane],axis=0)\n",
    "    # df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    # df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords = df_topic_keywords.transpose()\n",
    "df_topic_keywords.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_topic_keywords.to_csv(product['lda_topics_disaster_type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "duration": 241.671606,
   "end_time": "2022-08-04T04:26:53.028875",
   "exception": null,
   "input_path": "C:\\Users\\gillrobe\\AppData\\Local\\Temp\\tmpi7cs1cef.ipynb",
   "output_path": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\topic_modeling_disaster_type_final.ipynb",
   "parameters": {
    "product": {
     "lda_model_earthquake": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\lda_model_earthquake.pkl",
     "lda_model_fire": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\lda_model_fire.pkl",
     "lda_model_flood": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\lda_model_flood.pkl",
     "lda_model_hurricane": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\lda_model_hurricane.pkl",
     "lda_topics_disaster_type": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\lda_topics_disaster_type.csv",
     "nb": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\topic_modeling_disaster_type_final.ipynb"
    },
    "random_seed": 42,
    "upstream": {
     "vectorizer_countVec": {
      "nb": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\vectorizer_countVec.ipynb",
      "vectorizer": "C:\\Users\\gillrobe\\DataScience\\umads_697_data_medics\\pipeline\\output\\vectorizer_countVec.pkl"
     }
    }
   },
   "start_time": "2022-08-04T04:22:51.357269"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}